{"version":3,"sources":["webpack:///../src/ml.mdx"],"names":["_frontmatter","layoutProps","MDXLayout","DefaultLayout","MDXContent","components","props","mdxType","parentName","isMDXComponent"],"mappings":"uPAQaA,G,UAAe,S,4LAC5B,IAAMC,EAAc,CAClBD,gBAEIE,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGC,EACF,8BACD,OAAO,YAACJ,EAAD,eAAeD,EAAiBK,EAAhC,CAAuCD,WAAYA,EAAYE,QAAQ,cAG5E,iBAAQ,CACN,GAAM,6CADR,8CAGA,2OAA0N,0BAAYC,WAAW,KAAvB,SAA1N,uKACA,iBAAQ,CACN,GAAM,aADR,aAGA,mRACA,2bACA,inBACA,+VACA,4GACA,yGAAwF,0BAAYA,WAAW,KAAvB,SAAxF,OACA,iBAAQ,CACN,GAAM,oBADR,oBAGA,qBAAG,sBAAQA,WAAW,KAAnB,sBAAsD,kBAAIA,WAAW,MAAxE,KACF,sBAAQA,WAAW,KAAnB,SADE,4CACoF,kBAAIA,WAAW,MADnG,KAEF,sBAAQA,WAAW,KAAnB,UAFE,uBAGA,sBACE,kBAAIA,WAAW,MAAf,QACA,kBAAIA,WAAW,MAAf,SACA,kBAAIA,WAAW,MAAf,OACA,kBAAIA,WAAW,MAAf,YACA,kBAAIA,WAAW,MAAf,aACA,kBAAIA,WAAW,MAAf,OACA,kBAAIA,WAAW,MAAf,QACA,kBAAIA,WAAW,MAAf,aACA,kBAAIA,WAAW,MAAf,eAEF,iBAAQ,CACN,GAAM,kBADR,oBAGA,8LACA,iBAAQ,CACN,GAAM,sBADR,uBAGA,mKACA,iBAAQ,CACN,GAAM,OADR,OAGA,sMACA,iBAAQ,CACN,GAAM,QADR,QAGA,0UACA,iBAAQ,CACN,GAAM,wBADR,yBAGA,yIACA,8RACA,+XACA,qBAAG,sBAAQA,WAAW,KAAnB,YAAH,0E,0LAKJJ,EAAWK,gBAAiB","file":"component---src-ml-mdx-3b84f0fb3ce7be7abbaf.js","sourcesContent":["import * as React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\n\nimport DefaultLayout from \"/home/runner/work/openvtuber/openvtuber/docs/node_modules/gatsby-theme-docz/src/base/Layout.js\";\nexport const _frontmatter = {};\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <h1 {...{\n      \"id\": \"facial-detection-tracking-and-landmarking\"\n    }}>{`Facial Detection, Tracking and Landmarking`}</h1>\n    <p>{`The ML module performs facial detection, tracking, and landmarking (in that order) on the image input it receives. One can find all the functions related to image and facial processing in the Inference class in `}<inlineCode parentName=\"p\">{`ml.py`}</inlineCode>{`. The core logic is within the infer_image function at the bottom. It takes an image and transforms it, producing a tuple containing measurements about the face.  `}</p>\n    <h2 {...{\n      \"id\": \"inference\"\n    }}>{`Inference`}</h2>\n    <p>{`The inference process contains four steps, the first step is the initialisation process. The inference module relies heavily on dlib functions for much of its actual core actions, many of which need prior initialisation or preparation of some sort.  `}</p>\n    <p>{`The next part is where it actually takes in an image and begins the processing. It will look for a face in the image. If a face is not found, it will not go through with the remaining steps. Dlib functions performs the calculations for this part and this is probably the slowest part of the algorithm. Although there are other choices available online for this part, dlib is one of the most well known and most used.  `}</p>\n    <p>{`If it finds a face, it will take the cut out of the image that contains just the face and record its position. The algorithm does this so that it does not have to perform facial detection on every frame. This is because by recording past frames, we can actually use linear extrapolation to predict future face locations. Since facial detection is the slowest part in this entire algorithm, by being able to skip it occasionally, you can see noticeable performance boosts. In the current implementation, the algorithm skips every second frame and replaces it with this linear extrapolation approach.  `}</p>\n    <p>{`Once the algorithm detects a face, it crops and isolates the face so that it can then perform facial landmarking. This increases the accuracy and speed of the facial landmarking procedure tremendously. This process also takes advantage of an algorithm available through dlib that performs facial landmarking using 68 points.  `}</p>\n    <p>{`With the face landmarked, one can now use the points to calculate facial metrics.  `}</p>\n    <p>{`The entire process and algorithm is in the infer_image function at the bottom of `}<inlineCode parentName=\"p\">{`ml.py`}</inlineCode>{`.  `}</p>\n    <h2 {...{\n      \"id\": \"inference-output\"\n    }}>{`Inference Output`}</h2>\n    <p><strong parentName=\"p\">{`infer_image(image)`}</strong><br parentName=\"p\"></br>{`\n`}<strong parentName=\"p\">{`Input`}</strong>{`: 2D numpy array that represents an image`}<br parentName=\"p\"></br>{`\n`}<strong parentName=\"p\">{`Output`}</strong>{`: tuple of elements`}</p>\n    <ul>\n      <li parentName=\"ul\">{`roll`}</li>\n      <li parentName=\"ul\">{`pitch`}</li>\n      <li parentName=\"ul\">{`yaw`}</li>\n      <li parentName=\"ul\">{`ear_left`}</li>\n      <li parentName=\"ul\">{`ear_right`}</li>\n      <li parentName=\"ul\">{`mar`}</li>\n      <li parentName=\"ul\">{`mdst`}</li>\n      <li parentName=\"ul\">{`left_iris`}</li>\n      <li parentName=\"ul\">{`right_iris`}</li>\n    </ul>\n    <h3 {...{\n      \"id\": \"roll-pitch-yaw\"\n    }}>{`Roll, Pitch, Yaw`}</h3>\n    <p>{`These refer to the orientation of the head (in degrees). All three measurements start at a base line where 0 degrees is the subject staring straight into the camera.`}</p>\n    <h3 {...{\n      \"id\": \"ear_left-ear_right\"\n    }}>{`ear_left, ear_right`}</h3>\n    <p>{`Eye aspect ratio or ear is a measurement of how open an eye is. Values usually range from 0 to 0.4 where closed is 0 and very open is 0.4.`}</p>\n    <h3 {...{\n      \"id\": \"mar\"\n    }}>{`mar`}</h3>\n    <p>{`Mouth aspect ratio or mar is a measurement of how open the mouth is. Values usually range from 0 to 1 (although it will sometimes cross 1) where closed is 0 and open is 1.  `}</p>\n    <h3 {...{\n      \"id\": \"mdst\"\n    }}>{`mdst`}</h3>\n    <p>{`Mouth distance or mdst is a measure of the horizontal length of the mouth. (Think of it like a very wide smile versus a puckered kissing action). Values will usually range from 0.2 to 0.5 (although it may cross those boundaries on either side) where 0.2 is a very tight mouth and 0.5 is a very wide mouth.`}</p>\n    <h3 {...{\n      \"id\": \"left_iris-right_iris\"\n    }}>{`left_iris, right_iris`}</h3>\n    <p>{`These two iris measurements are not actually numbers but instead an array or collection of four other numbers.  `}</p>\n    <p>{`The first two of which are the raw x and y coordinates of the iris in relation to the rest of the image. These two numbers are probably not going to be very useful for most cases except for debugging. The coordinate (0, 0) is the upper left hand of the image.  `}</p>\n    <p>{`The next two of which are the left and up ratio values of the pupil. These represent how much to any direction the iris is pointing. The left ratio of the eye determines how much to the left the eye is pointing where 0 is very left and 1 is very right. The up ratio of the eye determines how upwards the eye is pointing where 0 is very down and 1 is very up.`}</p>\n    <p><strong parentName=\"p\">{`*`}{`Note`}</strong>{`: If it does not find a face, the function returns None instead.`}</p>\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"sourceRoot":""}